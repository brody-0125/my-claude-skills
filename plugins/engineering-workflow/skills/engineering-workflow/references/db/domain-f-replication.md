# Distributed Database: Replication & Consensus Reference
<!-- Agent: f1-replication-designer -->
<!-- Scope: Replication topologies, consensus protocols, rebalancing strategies -->
<!-- Split from: domain-f-distributed.md -->

> Static reference for the **f1-replication-designer** agent.
> Covers replication topologies, consensus protocols, rebalancing strategies,
> and practical configuration for MySQL/PostgreSQL replication setups.

---

## 1. Replication Topologies

### Single-Leader (Primary-Secondary)
```
         ┌──────────┐
         │  Leader   │  <- All writes go here
         │ (Primary) │
         └────┬──┬───┘
              │  │
     ┌────────┘  └────────┐
┌────▼─────┐        ┌─────▼────┐
│ Follower │        │ Follower │   <- Serve reads (eventually consistent)
└──────────┘        └──────────┘
```

- **Write path**: Client -> Leader -> WAL/binlog -> Replicas apply async or sync
- **Failover**: Promote follower to leader (manual or automatic)
- **Replication lag**: Async: 10ms-seconds; Semi-sync: <10ms; Sync: 0 (higher write latency)
- **Use case**: Standard OLTP, MySQL/PostgreSQL default, read-scaling

**Semi-Synchronous**: Leader waits for at least one replica ACK before confirming commit. Trade-off: +1 RTT write latency vs durability guarantee.

#### MySQL Replication Configuration

```sql
-- PRIMARY: /etc/mysql/mysql.conf.d/mysqld.cnf
-- [mysqld]
-- server-id       = 1
-- log_bin         = /var/log/mysql/mysql-bin.log
-- binlog_format   = ROW
-- gtid_mode       = ON
-- enforce_gtid_consistency = ON
-- rpl_semi_sync_master_enabled = 1
-- rpl_semi_sync_master_wait_for_slave_count = 1

CREATE USER 'repl_user'@'%' IDENTIFIED BY '...';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;
SHOW MASTER STATUS;  -- note File and Position
```

```sql
-- REPLICA: server-id=2, read_only=ON, gtid_mode=ON
-- rpl_semi_sync_slave_enabled = 1
CHANGE MASTER TO MASTER_HOST='primary-host', MASTER_USER='repl_user',
    MASTER_PASSWORD='...', MASTER_AUTO_POSITION=1;
START SLAVE;
SHOW SLAVE STATUS\G   -- check Seconds_Behind_Master, Slave_IO_Running, Slave_SQL_Running
```

#### PostgreSQL Streaming Replication Configuration

```ini
# postgresql.conf on PRIMARY
wal_level = replica
max_wal_senders = 10
synchronous_standby_names = 'replica1'
synchronous_commit = remote_apply
```

```bash
# On REPLICA — bootstrap from primary
pg_basebackup -h primary-host -U repl_user -D /var/lib/postgresql/data \
  --checkpoint=fast --wal-method=stream --slot=replica1_slot -R
# -R flag auto-creates standby.signal + primary_conninfo in postgresql.auto.conf
```

```ini
# postgresql.auto.conf on REPLICA (auto-generated by -R)
primary_conninfo = 'host=primary-host user=repl_user password=... application_name=replica1'
primary_slot_name = 'replica1_slot'
```

#### Monitoring Replication Health

```sql
-- PostgreSQL: check replication lag on PRIMARY
SELECT client_addr, state, sent_lsn, write_lsn, flush_lsn, replay_lsn,
       (sent_lsn - replay_lsn) AS replay_lag_bytes
FROM pg_stat_replication;

-- MySQL: check on REPLICA
SHOW SLAVE STATUS\G
-- Key fields: Seconds_Behind_Master, Slave_IO_Running, Slave_SQL_Running
-- Semi-sync status on PRIMARY:
SHOW STATUS LIKE 'Rpl_semi_sync_master%';
```

### Multi-Leader (Active-Active)
```
┌──────────┐         ┌──────────┐
│ Leader A │ <-----> │ Leader B │   <- Both accept writes
│ (Region1)│         │ (Region2)│   <- Bidirectional replication
└──────────┘         └──────────┘
```

- **Use case**: Multi-region deployments, offline-capable clients
- **Challenge**: Write conflicts when same data modified in different regions
- **Conflict detection**: LWW, version vectors, or custom resolution
- **Anti-pattern**: Avoid for single-region deployments (complexity not justified)

```sql
-- MySQL Group Replication (Multi-Primary Mode)
SET GLOBAL group_replication_single_primary_mode = OFF;
SET GLOBAL group_replication_enforce_update_everywhere_checks = ON;
START GROUP_REPLICATION;
SELECT MEMBER_HOST, MEMBER_ROLE FROM performance_schema.replication_group_members;
```

### Leaderless (Dynamo-Style)
```
      Client
     /  |  \
┌──▼┐ ┌▼──┐ ┌──▼┐
│N1 │ │N2 │ │N3 │   W + R > N ensures overlap (quorum)
└───┘ └───┘ └───┘
```

- **Quorum**: N=3, W=2, R=2 (majority quorum) — reads see latest write
- **Read repair**: Stale value detected during read -> update stale node
- **Anti-entropy**: Background Merkle tree comparison to reconcile differences
- **Examples**: Cassandra, DynamoDB, Riak

---

## 2. Consensus Protocols

### Raft

**Overview**: Understandable consensus protocol. Decomposes into leader election, log replication, and safety.

**Leader Election**: Follower timeout (150-300ms) -> candidate -> majority vote -> leader. Heartbeats prevent new elections.

**Log Replication**: Client -> leader appends log -> AppendEntries RPC to followers -> majority ACK -> commit -> apply to state machine.

**Safety Properties**
- Election Safety: At most one leader per term
- Leader Append-Only: Never overwrites or deletes entries
- Log Matching: Same index+term implies identical preceding entries
- State Machine Safety: Applied entry at index is consistent across servers

**Performance**: Write latency 1 RTT; throughput limited by leader; cluster 3/5/7 nodes; failover ~200-500ms.

```yaml
# etcd Raft configuration (etcd.conf.yml)
name: 'node1'
data-dir: '/var/lib/etcd'
listen-peer-urls: 'https://10.0.1.1:2380'
listen-client-urls: 'https://10.0.1.1:2379'
initial-cluster: 'node1=https://10.0.1.1:2380,node2=https://10.0.1.2:2380,node3=https://10.0.1.3:2380'
heartbeat-interval: 100       # ms
election-timeout: 1000        # ms
snapshot-count: 10000
```

### Paxos

**Overview**: Original consensus protocol by Lamport. More general but harder to understand and implement than Raft.

**Roles**: Proposer, Acceptor, Learner (a node can have multiple roles).

**Basic Paxos (Single-Decree)**
1. **Prepare (Phase 1a)**: Proposer sends Prepare(n) with proposal number n
2. **Promise (Phase 1b)**: Acceptor promises not to accept proposals < n; returns any accepted value
3. **Accept (Phase 2a)**: Proposer sends Accept(n, value)
4. **Accepted (Phase 2b)**: Acceptor accepts if no higher-numbered prepare received

**Multi-Paxos**: Optimization where stable leader skips Phase 1 for subsequent proposals, reducing to single-phase commit.

**Raft vs Paxos**

| Aspect | Raft | Paxos |
|--------|------|-------|
| Understandability | High | Low |
| Leader requirement | Yes (strong) | Optional (Multi-Paxos) |
| Log management | Contiguous, no gaps | May have gaps |
| Production use | etcd, CockroachDB, TiKV | Chubby, Spanner, Megastore |
| Latency (steady) | 1 RTT | 1 RTT (stable leader) |

---

## 3. Rebalancing Strategies

### Fixed Number of Partitions
- Pre-allocate many more partitions than nodes (e.g., 1000 on 10 nodes)
- Adding node: move some partitions from existing nodes
- Disadvantage: Must choose count upfront; too few = uneven, too many = overhead
- Used by: Elasticsearch, Riak, CouchDB

### Dynamic Splitting
- Split when size exceeds threshold; merge when below threshold
- Adapts to data volume; brief unavailability during split
- Used by: HBase (region splitting), CockroachDB (512MB default)

### Consistent Hashing
```
     Node A          Node B
       │                │
   ────●────────────────●────────────
   0          Node C             2^32
               │
   ────────────●─────────────────────
```
- Key belongs to nearest clockwise node; ~1/N keys move on scale-out
- Virtual nodes for even distribution
- Used by: DynamoDB, Cassandra

---

## 4. Case Study: Pinterest

**Sharding Design** (2012-2013)
- 8192 virtual shards on MySQL/InnoDB, mapped to fewer physical instances
- ID embeds shard: `[shard_id:16][type:16][local_id:32]`
- All related data on same shard (locality); multi-shard queries avoided by design

```python
# Pinterest-style shard routing
SHARD_MAP = {0: 'mysql-host-01', 1: 'mysql-host-01', ..., 8191: 'mysql-host-64'}

def get_shard_id(object_id: int) -> int:
    return (object_id >> 48) & 0xFFFF  # top 16 bits

def route_query(object_id: int) -> str:
    return SHARD_MAP[get_shard_id(object_id)]
```

**Resharding**: Assign virtual shards to new physical hosts via MySQL replication + cutover. Zero-downtime dual-read during migration. p99 <5ms for single-shard queries.

---

## 5. Decision Matrix

### Replication Topology Selection

| Factor | Single-Leader | Multi-Leader | Leaderless |
|--------|---------------|-------------|------------|
| Write throughput | Limited to leader | Higher (multi-writer) | Highest (any node) |
| Consistency | Strong (sync) / Eventual (async) | Eventual + conflict resolution | Tunable quorum |
| Failover time | 1-30s (auto promotion) | Near-zero | None needed |
| Conflict handling | None | Required (LWW, CRDTs) | Required (read repair) |
| Complexity | Low | High | Medium |
| Best for | Standard OLTP | Multi-region | High-availability KV |

### Consensus Protocol Selection

| Requirement | Protocol | Rationale |
|-------------|---------|-----------|
| New system, small team | Raft | Easier to understand and debug |
| Google-scale | Multi-Paxos | Battle-tested in Spanner, Chubby |
| Kubernetes / service discovery | Raft (etcd) | De-facto standard |
| Byzantine fault tolerance | PBFT or HotStuff | Raft/Paxos assume non-Byzantine |

### Rebalancing Strategy Selection

| Factor | Fixed Partitions | Dynamic Splitting | Consistent Hashing |
|--------|-----------------|-------------------|-------------------|
| Upfront planning | Required | Minimal | Minimal |
| Adapts to data skew | No | Yes (split hot) | Partial (vnodes) |
| Data movement on scale-out | ~1/N partitions | Only split ranges | ~1/N keys |
| Best for | Elasticsearch, Kafka | CockroachDB, HBase | Cassandra, DynamoDB |

### Anti-Patterns

| Anti-Pattern | Problem | Better Approach |
|-------------|---------|----------------|
| Sync replication across regions | 100-200ms write latency | Semi-sync local + async cross-region |
| Too few initial partitions | Expensive resharding | Over-provision virtual partitions (1024+) |
| No failover runbook | Manual promotion errors | Automated failover with health checks |
| Ignoring replication lag | Stale reads | Read-from-primary for critical reads |
| Geo-replication without conflict strategy | Silent data loss | Define conflict resolution before deploying |

---

## 6. References

1. **Ongaro & Ousterhout (2014)** -- "In Search of an Understandable Consensus Algorithm (Raft)" -- USENIX ATC'14
2. **Lamport (1998)** -- "The Part-Time Parliament (Paxos)" -- ACM TOCS
3. **DeCandia et al. (2007)** -- "Dynamo: Amazon's Highly Available Key-Value Store" -- SOSP'07
4. **Corbett et al. (2013)** -- "Spanner: Google's Globally-Distributed Database" -- ACM TOCS

---

*Last updated: 2025-05. Sources include vendor documentation, engineering blogs, and peer-reviewed publications.*

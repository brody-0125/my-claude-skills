# Orchestration Protocol

> Defines how Plugin Introspector commands invoke agents, pass data between them,
> and compose multi-agent analysis pipelines.

## Session Directory Resolution

Every command MUST resolve the session directory before accessing data.
See [SKILL.md Pre-Execution](../SKILL.md) for user-facing procedure.

### Available Data Files

After resolving SESSION_DIR, these files are available:

| File | Always exists | Content |
|------|---------------|---------|
| `meta.json` | Yes | Session metadata (git info, working dir) |
| `tool_traces.jsonl` | Yes | Pre/post tool trace records with input_summary |
| `api_traces.jsonl` | Yes | API request/response metrics (may be empty if Notification hook unavailable) |
| `otel_traces.jsonl` | Yes | OTel-compatible span records |
| `stats.json` | Yes | Aggregated session statistics |
| `evaluation.json` | No | Created by `evaluate` command |

### Cross-Session Data

| File | Location | Content |
|------|----------|---------|
| `session_history.jsonl` | `~/.claude/plugin-introspector/` | One record per completed session (with phase/waste metrics) |
| `evaluation_history.jsonl` | `~/.claude/plugin-introspector/` | One record per `evaluate` run (scores, phase_breakdown, improvements_active) |
| `alerts.jsonl` | `~/.claude/plugin-introspector/` | Anomaly detection alerts |
| `improvement_log.jsonl` | `~/.claude/plugin-introspector/` | Applied improvement history |

### Plugin Profile Data

| File | Location | Content |
|------|----------|---------|
| `profile.json` | `~/.claude/plugin-introspector/plugin-profiles/{plugin}/` | Workflow structure extracted from SKILL.md |
| `phase-baselines.json` | `~/.claude/plugin-introspector/plugin-profiles/{plugin}/` | Per-phase statistical baselines |
| `learned-patterns.jsonl` | `~/.claude/plugin-introspector/plugin-profiles/{plugin}/` | Mined improvement patterns |

---

## Collection Tiers

The data collection layer supports two tiers, automatically detected at session start:

| Tier | Description | Detection | Hook Behavior |
|------|-------------|-----------|---------------|
| **Tier 0** | Pure hooks | Default (no collector) | Full: tool traces + OTel spans + API traces + stats |
| **Tier 1** | Hooks + OTel Collector | `otel-collector.pid` active OR `otel-export/` has data | Reduced: tool traces + API traces + stats (OTel spans from native; api-trace.sh always runs) |

### Tier Detection Logic

```
1. Check ~/.claude/plugin-introspector/otel-collector.pid
   - If file exists AND process alive → Tier 1
2. Check ~/.claude/plugin-introspector/otel-export/ directory
   - If exists AND non-empty → Tier 1
3. Otherwise → Tier 0
```

### Tier 0: Pure Hooks (Default)

All data generated by hook scripts:
- `meta.json` / `stats.json` — Initialized by SessionStart, finalized by SessionEnd/Stop
- `tool_traces.jsonl` — PreToolUse/PostToolUse/PostToolUseFailure/SubagentStop hooks
- `otel_traces.jsonl` — Generated by PostToolUse/PostToolUseFailure/SubagentStop hooks
- `api_traces.jsonl` — Notification hook
- `session_history.jsonl` — Appended by SessionEnd hook
- `alerts.jsonl` — Appended by Stop hook (anomaly detection)

### Tier 1: Hooks + OTel Collector

Native Claude Code telemetry (`CLAUDE_CODE_ENABLE_TELEMETRY=1`) sent to OTel Collector,
which exports OTLP JSON via File Exporter to `~/.claude/plugin-introspector/otel-export/`.

Hook behavior changes at Tier 1:
- `PostToolUse`/`PostToolUseFailure`: **Skip** OTel span generation (native handles it)
- `SubagentStop`: **Skip** OTel span generation (native handles it)
- `Notification`: Still records (provides real-time stats compatibility)
- `SessionEnd`: Triggers `merge-otel-data.sh` before stats finalization

At session end, `merge-otel-data.sh`:
1. Reads OTLP JSON from `otel-export/*.json`
2. Filters spans by session timeframe
3. Converts to our `otel_traces.jsonl` format (with `_source: "native_otel"` tag)
4. Appends to session's `otel_traces.jsonl`

### Tier 1 Setup

```
1. Run: scripts/setup-otel-collector.sh install    (downloads ~100MB+ binary)
2. Run: scripts/setup-otel-collector.sh start      (starts collector)
3. Set environment:
   export CLAUDE_CODE_ENABLE_TELEMETRY=1
   export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
4. Verify: scripts/setup-otel-collector.sh status
```

### Hook Externalization

All hooks in `plugin.json` delegate to external scripts via:
```
PI_S="${INTROSPECTOR_SCRIPTS:-${CLAUDE_PROJECT_DIR:-.}/plugins/plugin-introspector/skills/plugin-introspector/scripts}"
bash "$PI_S/{script-name}.sh" 2>/dev/null || true
```

Override script location via `INTROSPECTOR_SCRIPTS` environment variable if the plugin
is installed outside the project directory.

---

## Data Collection Hooks

Plugin Introspector uses Claude Code hooks to collect execution data in real-time.
The hook pipeline follows the session lifecycle:

### Hook Execution Pipeline

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           SESSION LIFECYCLE                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  SessionStart ─────────────────────────────────────────────────► SessionStop│
│       │                                                              │      │
│       ▼                                                              │      │
│  ┌─────────┐                                                         │      │
│  │ Init    │  Creates session dir, meta.json, initializes stats      │      │
│  └─────────┘                                                         │      │
│                                                                      │      │
│       ┌──────────────── Tool/Subagent Cycle ───────────────┐        │      │
│       │                                                     │        │      │
│       │  PreToolUse ──► [Tool Execution] ──► PostToolUse   │        │      │
│       │       │                                    │        │        │      │
│       │       ▼                                    ▼        │        │      │
│       │  tool_traces.jsonl (pre)     tool_traces.jsonl (post)       │      │
│       │                              otel_traces.jsonl              │      │
│       │                                                     │        │      │
│       │  [Subagent spawned] ──────────► SubagentStop        │        │      │
│       │                                      │              │        │      │
│       │                                      ▼              │        │      │
│       │                          tool_traces.jsonl          │        │      │
│       │                           otel_traces.jsonl         │        │      │
│       │                                                     │        │      │
│       │  Notification ──► api_traces.jsonl                  │        │      │
│       │                                                     │        │      │
│       └─────────────────────────────────────────────────────┘        │      │
│                                                                      │      │
│       ▼                                                              ▼      │
│  ┌─────────┐                                                   ┌─────────┐  │
│  │SessionEnd│  Finalizes stats, appends session_history       │  Stop   │  │
│  └─────────┘  (Tier 1: triggers merge-otel-data.sh)           │ Anomaly │  │
│                                                                │ detect  │  │
│                                                                └─────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Hook-to-Script Mapping

| Hook Event | Script | Purpose | Output Files |
|------------|--------|---------|--------------|
| `SessionStart` | `session-start.sh` | Initialize session directory, create meta.json, start stats tracking | `meta.json`, `stats.json` |
| `PreToolUse` | `security-check.sh` | Command risk classification, DLP, sensitive write detection (requires `PI_ENABLE_SECURITY=1`) | `security_events.jsonl`, `alerts.jsonl` |
| `PreToolUse` | `pre-tool-trace.sh` | Log tool invocation start, estimate input tokens | `tool_traces.jsonl` (pre record) |
| `PostToolUse` | `post-tool-trace.sh` | Log tool completion, generate OTel span (Tier 0), update stats | `tool_traces.jsonl` (post record), `otel_traces.jsonl` |
| `PostToolUseFailure` | `tool-failure-trace.sh` | Log tool failure with error snippet | `tool_traces.jsonl` (failure record), `otel_traces.jsonl` |
| `SubagentStop` | `subagent-trace.sh` | Log subagent completion, generate OTel span (Tier 0). Note: SubagentStart event does not exist in Claude Code — start_time is unavailable. | `tool_traces.jsonl` (subagent record), `otel_traces.jsonl` |
| `Notification` | `api-trace.sh` | Record API request/response metrics | `api_traces.jsonl` |
| `SessionEnd` | `session-end.sh` | Finalize stats via delta aggregation, append session_history, merge OTel data (Tier 1) | `stats.json`, `session_history.jsonl` |
| `Stop` | `session-stop.sh` | Anomaly detection, alert generation, aggregate stats, auto-rotate (optional) | `alerts.jsonl`, `aggregates.json`, `stats.json` |

### Hook Constraints

All hook scripts MUST comply with meta-rules:

1. **Execution time:** Complete within 50ms (hooks block Claude Code execution)
2. **Error handling:** Use `set -euo pipefail` and `|| true` fallback
3. **Dependencies:** bash + jq only (no Python, Node.js, or external tools)
4. **I/O pattern:** Append-only writes to JSONL files
5. **Failure isolation:** Hook failures must not break the main workflow

### Security Hooks (Optional)

When `PI_ENABLE_SECURITY=1`, additional security checks run in hooks:

| Hook | Security Function | Script |
|------|-------------------|--------|
| `PreToolUse` | Command risk assessment, DLP input scan, CRITICAL command blocking | `security-check.sh` |
| `PostToolUse` | DLP output scan | `post-tool-trace.sh` |

Security events are logged to `security_events.jsonl` in the session directory.

---

## Agent Invocation Pattern

All analysis agents are invoked via the **Task** tool with `subagent_type=general-purpose`.
The orchestrating SKILL reads the agent .md file and constructs a prompt that includes:

1. The agent definition (from agents/*.md)
2. The relevant data (read from session JSONL files)
3. The expected output format

### Single-Agent Command Pattern

Used by: `analyze`, `tokens`, `api`, `context`, `evaluate`, `alerts`

```
Step 1: Resolve session directory (see above)
Step 2: Read the relevant JSONL data files using Read tool
Step 3: Apply data truncation (see Data Size Management below)
Step 4: Read the agent definition (agents/{agent-name}.md) using Read tool
Step 5: Invoke Task tool with prompt constructed from template (see Prompt Template below)
        - subagent_type: "general-purpose"
        - model: as specified in agent's "Model Assignment" section
Step 6: Parse agent output (structured JSON)
Step 7: Display results to user
Step 8: If agent produces evaluation.json, write it to SESSION_DIR
```

### Task Prompt Template

When invoking an agent via Task, construct the prompt as follows:

```
[Full agent definition content from agents/{name}.md]

---

## Session Data

Session ID: {session_id}
Session Duration: {duration_ms}ms
Total Tool Calls: {tool_calls}
Total Tokens (est): {total_tokens_est}
Errors: {errors}
Error Rate: {error_rate}%

### tool_traces.jsonl ({N} records)
{tool_traces content, truncated per Data Size Management}

### api_traces.jsonl ({M} records)
{api_traces content}

### stats.json
{stats.json content}

[Include otel_traces.jsonl only if needed by the specific agent]

---

Produce your output as a single JSON object following the Output Format in the agent definition above.
```

### Data Size Management

JSONL files can grow large in long sessions. Apply these truncation rules before passing to agents:

| File | Max Records | Truncation Strategy |
|------|-------------|---------------------|
| `tool_traces.jsonl` | 500 lines | Keep last 500 (`tail -500`). Prepend note if truncated. |
| `api_traces.jsonl` | 100 lines | Keep all (typically small) |
| `otel_traces.jsonl` | 500 lines | Keep last 500 (`tail -500`). Prepend note if truncated. |
| `session_history.jsonl` | 20 lines | Keep last 20 sessions (`tail -20`) |
| `evaluation_history.jsonl` | 20 lines | Keep last 20 evaluations (`tail -20`) |
| `improvement_log.jsonl` | 50 lines | Keep all (typically small) |
| `learned-patterns.jsonl` | 50 lines | Keep all (per-plugin, typically small) |

If truncated, prepend: `[Truncated: showing last {N} of {total} records]`

### Target Plugin File Selection

When passing target plugin files to agents (`improve`, `optimize`, `context`), large plugins may exceed practical context limits. Apply this selection strategy:

```
1. Read target_plugin.json for the full component catalog
2. Estimate total tokens: sum of (file_size_chars / 4) for all cataloged files
3. If total_tokens ≤ 20,000:
   → Include ALL component files in the agent prompt
4. If total_tokens > 20,000:
   → Apply priority-based selection:
     a) ALWAYS include: SKILL.md, plugin.json (core definitions)
     b) Include files REFERENCED in analysis results:
        - Files mentioned in workflow-analyzer recommendations
        - Files flagged by token-optimizer waste_sources
        - Files cited in quality-evaluator improvement_signals
     c) Include files matching the specific command:
        - 'context': include all skills/ and agents/ (for static cost measurement)
        - 'improve': include files from (b) + their containing directory siblings
        - 'optimize --component {file}': include only the target component + SKILL.md
     d) EXCLUDE by default: references/ (consumed by plugin, not modified), templates/
     e) For remaining files, include up to the 20,000 token budget
5. Note in prompt: "[File selection: {N} of {total} files included. Excluded: {list}]"
```

### Multi-Agent Pipeline Pattern

Used by: `report`, `improve`, `optimize`

```
Step 1: Resolve session directory
Step 2: Read all JSONL data files once (reuse across agents)
Step 3: Apply data truncation
Step 4: For 'report': Run independent agents in parallel where possible:
        - Parallel batch 1: workflow-analyzer, token-optimizer, api-tracker, context-auditor
        - Sequential after batch 1: quality-evaluator (may use earlier results)
        For 'improve':
        - Load cross-session data: evaluation_history.jsonl, improvement_log.jsonl
        - Load plugin profile data: profile.json, phase-baselines.json, learned-patterns.jsonl
        - Parallel batch 1: workflow-analyzer, token-optimizer, context-auditor
        - Parallel batch 2: anomaly-detector (independent), quality-evaluator
        - Sequential: improvement-generator (receives all upstream results + cross-session + profile)
        - Also pass improvement-pipeline.md as procedure reference
        For 'optimize':
        - Read evaluation.json + evaluation_history.jsonl + improvement_log.jsonl + target component
        - Pass to auto-optimizer
Step 5: Aggregate outputs into combined context
Step 6: Display final results
```

---

## Target Plugin Discovery

When commands specify `--target {plugin-name}`, the introspector needs to locate
that plugin's files for context-auditor and improvement-generator.

### Plugin Discovery Procedure

```
1. Search for plugin directory:
   a) Check: {working_dir}/plugins/{plugin-name}/
   b) Check: {working_dir}/../plugins/{plugin-name}/
   c) Check marketplace.json for source path
2. Once found, catalog the plugin's component files:
   - .claude-plugin/plugin.json (hooks)
   - skills/*/SKILL.md (skill definitions)
   - skills/*/agents/*.md (agent definitions)
   - skills/*/resources/*.md (protocol documents)
   - skills/*/references/*.md (reference documents — read-only, consumed by plugin)
   - skills/*/scripts/*.sh (scripts)
   - skills/*/templates/* (templates)
3. Store discovered paths in SESSION_DIR/target_plugin.json:
   {
     "plugin_name": "sub-kopring-engineer",
     "plugin_root": "/path/to/plugins/sub-kopring-engineer",
     "total_chars": 264000,
     "total_tokens_est": 66000,
     "components": {
       "skills": ["skills/sub-kopring-engineer/SKILL.md"],
       "agents": [],
       "resources": ["skills/.../resources/plan-protocol.md", ...],
       "references": ["skills/.../references/code-style-guide.md", ...],
       "scripts": ["skills/.../scripts/verify-conventions.sh", ...],
       "templates": ["skills/.../templates/plan-template.md", ...],
       "hooks": ".claude-plugin/plugin.json"
     }
   }
4. Check/generate plugin profile:
   a) Check: plugin-profiles/{plugin-name}/profile.json exists
   b) If exists: compare source_hash with md5 of current main SKILL.md
   c) If missing or hash mismatch (stale):
      - Read the plugin's main SKILL.md content
      - Invoke plugin-profiler agent (Task, haiku):
        pass SKILL.md content + component catalog
      - Write result to plugin-profiles/{plugin-name}/profile.json
      - Create empty phase-baselines.json and learned-patterns.jsonl if missing
   d) Store profile path reference in target_plugin.json
```

---

## Plugin Identification from Traces

To determine which plugin was active during a session, analyze `tool_traces.jsonl`:

### Heuristic Detection

```
1. Scan for Skill tool invocations:
   - input_summary contains a known skill name → direct identification

2. Scan input_summary fields for plugin-specific paths:
   - For each known plugin profile in plugin-profiles/:
     load profile.json detection_patterns
     match against input_summary fields
   - Match by plugin directory name in file paths (e.g., "sub-kopring-engineer")

3. Scan Read tool targets:
   - Reading resources/*.md from a known plugin path → identified

4. If no plugin identified: mark as "general session" (no specific plugin)
```

---

## Data Passing Between Agents

Agents communicate through structured JSON. The orchestrator (SKILL.md command handler)
is responsible for:

1. **Reading** data files and feeding them to agents as prompt context
2. **Parsing** each agent's JSON output
3. **Forwarding** relevant fields from one agent's output to the next agent's input

### Data Flow for `improve` command

```
              ┌──────────────────┐     ┌──────────────────────┐
              │  Read JSONL Data  │     │  Load Cross-Session   │
              │  (session traces) │     │  evaluation_history   │
              └────────┬─────────┘     │  improvement_log      │
                       │               │  plugin profile       │
          ┌────────────┼────────┐      │  learned-patterns     │
          ▼            ▼        ▼      └──────────┬───────────┘
  ┌────────────┐ ┌──────────┐ ┌──────────┐       │
  │ workflow-  │ │ token-   │ │ context- │       │
  │ analyzer   │ │ optimizer│ │ auditor  │       │
  └─────┬──────┘ └────┬─────┘ └────┬─────┘       │
        │              │            │              │
        ▼              ▼            ▼              │
  ┌───────────┐  ┌──────────┐ ┌──────────┐       │
  │ anomaly-  │  │quality-  │ │context   │       │
  │ detector  │  │evaluator │ │results   │       │
  │ (alerts)  │  └────┬─────┘ └────┬─────┘       │
  └─────┬─────┘       │            │              │
        │              │            │              │
        └──────────────┼────────────┘              │
                       │                           │
                       ▼                           ▼
           ┌──────────────────────────────────────────┐
           │        improvement-generator             │
           │  + cross-reference engine                │
           │  + ROI scoring (cost-tracking formula)   │
           │  + 3-layer pattern matching              │
           │  + historical validation                 │
           └────────────────┬─────────────────────────┘
                            │
                            ▼
           ┌──────────────────────────────────────────┐
           │  Proposals with ROI scores, quantified   │
           │  evidence, counterfactuals, diffs,       │
           │  meta-rules validation                   │
           └──────────────────────────────────────────┘
```

### Data Flow for `optimize` command

```
     ┌────────────────┐     ┌──────────────────┐
     │ evaluation.json │     │ target component │
     │ (from evaluate) │     │ (plugin file)    │
     └───────┬────────┘     └────────┬─────────┘
             │                       │
             └───────────┬───────────┘
                         │
                         ▼
              ┌───────────────────┐
              │  auto-optimizer   │
              │  (APE loop ×3)   │
              └─────────┬─────────┘
                        │
                        ▼
              ┌───────────────────┐
              │ Optimized version │
              │ with diff + score │
              └───────────────────┘
```

---

## Error Handling in Orchestration

| Situation | Action |
|-----------|--------|
| Session directory not found | Display "No session data found. Run a session with hooks enabled first." |
| JSONL file empty | Skip that data source, note in output: "No {type} data available" |
| Agent Task fails | Log error, continue with remaining agents, note partial results |
| Target plugin not found | Display "Plugin '{name}' not found. Available: {list}" |
| jq not available | Fall back to raw file display, warn about limited analysis |
